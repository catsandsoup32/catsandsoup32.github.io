<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://catsandsoup32.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://catsandsoup32.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-08T05:16:18+00:00</updated><id>https://catsandsoup32.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Schrödinger Bridge Diffusion</title><link href="https://catsandsoup32.github.io/notes/2025/SB-diffusion/" rel="alternate" type="text/html" title="Schrödinger Bridge Diffusion"/><published>2025-04-01T00:00:00+00:00</published><updated>2025-04-01T00:00:00+00:00</updated><id>https://catsandsoup32.github.io/notes/2025/SB-diffusion</id><content type="html" xml:base="https://catsandsoup32.github.io/notes/2025/SB-diffusion/"><![CDATA[<h2 id="kl-divergence">KL divergence</h2> <ul> <li>The <em>Kullback-Leibler</em> divergence is a non-symmetrical quantification of the difference between two probability distributions</li> <li>The KL divergence of $q(x)$ from $p(x)$ (where $x$ is a discrete random variable) measures the information lost if $q(x)$ were used to approximate $p(x)$</li> <li> <p>Quantified as the expected extra bits required to code samples of $p(x)$ using a code based on $q(x)$ rather than $p(x)$:</p> \[D(p(x) || q(x)) = \sum_{x\in X} p(x) \log \frac{p(x)}{q(x)}\] </li> </ul> <h3 id="entropy">Entropy</h3> <ul> <li>KL divergence is also termed relative entropy</li> <li> <p>The relationship between Shannon entropy, cross-entropy, and relative entropy can be written as follows (for discrete random variable $x$):</p> \[D(p(x) || q(x)) = Q(p(x) || q(x)) - H(p(x))\] <details> <summary>Proof (by definition)</summary> $$ \begin{align*} D(p(x) || q(x)) &amp;= \sum_{x\in X} p(x) [\log p(x) - \log q(x)] \\ &amp;= - \sum_{x\in X} p(x) \log q(x) + \sum_{x\in X} p(x) \log p(x) \\ &amp;= Q(p(x) || q(x)) - H(p(x)) \quad \Box \end{align*} $$ </details> </li> <li>In other words, the relative entropy (extra bits needed using predicted distribution) is the cross-entropy (total bits using predicted distribution) minus the Shannon entropy (inherent bits using true distribution)</li> </ul> <h2 id="diffusion">Diffusion</h2> <p>(TODO)</p> <h2 id="references">References</h2> <p>[1] https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf</p> <p>[2] https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[KL divergence, diffusion, optimal transport [WIP]]]></summary></entry></feed>
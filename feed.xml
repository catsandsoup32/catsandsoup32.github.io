<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://catsandsoup32.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://catsandsoup32.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-03T07:02:10+00:00</updated><id>https://catsandsoup32.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Schrödinger Bridge Diffusion</title><link href="https://catsandsoup32.github.io/notes/2025/SB-diffusion/" rel="alternate" type="text/html" title="Schrödinger Bridge Diffusion"/><published>2025-04-01T00:00:00+00:00</published><updated>2025-04-01T00:00:00+00:00</updated><id>https://catsandsoup32.github.io/notes/2025/SB-diffusion</id><content type="html" xml:base="https://catsandsoup32.github.io/notes/2025/SB-diffusion/"><![CDATA[<h2 id="kl-divergence">KL divergence</h2> <ul> <li>The <em>Kullback-Leibler</em> divergence is a non-symmetrical quantification of the difference between two probability distributions</li> <li>The KL divergence of $q(x)$ from $p(x)$ (where $x$ is a discrete random variable) measures the information lost if $q(x)$ were used to approximate $p(x)$</li> <li> <p>Mathematically,</p> \[D(p(x) || q(x)) = \sum_{x\in X} p(x) \ln \frac{p(x)}{q(x)}\] </li> </ul> <h3 id="smoothing">Smoothing:</h3> <ul> <li>In a machine learning context, $q(x)$ is a prediction, and so can easily be a tiny number</li> <li>To avoid a divergence value towards infinity, a small constant $\epsilon$ can be added</li> </ul> <h3 id="connection-to-shannon-entropy">Connection to Shannon entropy</h3> <p>https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[KL divergence, diffusion, optimal transport [WIP]]]></summary></entry></feed>